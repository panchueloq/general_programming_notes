// IMPORT // 
import pandas as pd


// READ //
df = pd.read_csv("<place/csv_file.csv>")


// DISPLAY COLUMNS AND ROWS //
pd.set_option('display.max_columns', <NUMBER>)
pd.set_option('display.max_rows', <NUMBER>)


// ATTRIBUTES //
df.shape 
df.columns (see all columns)


// SEE DATAFRAME //
df (or any name) to see default display
df.head(<optional_number>) to see first rows
df.tail(<optional_number>) to see last rows
df.info()


// CREATE DATA FRAME //
df = pd.DataFrame(<dict>)


// ACCESS COLUMNS //
df["col_name"]
df.col_name  (not recommended)
df["col_1", "col_2"]


// ACCESS ROWS //
df.iloc[]  (by index)
df.iloc[[0, 1]]  (list of rows)
df.iloc[[0, 1], 2]  (specific column)
df.loc[]  (by label)
df.loc[[0, 1]] 
df.loc[[0, 1], 'email'] 
df.loc[[0, 1], ['email', 'last']]  (columns displayed in order)


// SLICING //
df.loc[0:2, "col_2":"col_6"]   (slicing is inclusive here)


// ANALYSIS //
df['column_name'].value_counts()   (specific values count)


// SETTING INDEX //
df.set_index('col_name', inplace=True)   (inplace needed to save changes)
df.reset_index(inplace=True)

// To set index when reading //
df = pd.read_csv("<place/csv_file.csv>", index_col = "col_name")


// SORT ALPHABETICALLY //
schema_df.sort_index()
schema_df.sort_index(ascending=False)  (REVERSE)


// SHIFTING INDEX //
.shift(periods = x)


// FILTERING //
df['col_name'] == 'smth'
filt = (df['col_name'] == 'smth')  (brackets not necessary)
df[filt]  (apply)
df.loc[filt]   (also apply, recommended)
df.loc[filt, 'email']   (filter and get specific column)

filt = (df['last'] == 'Doe') & (df['first'] == 'John')   (AND filter)
filt = (df['last'] == 'Schafer') | (df['first'] == 'John')   (OR filter)

df.loc[~filt]   (Opposite filter)

high_salary = (df['Salary'] > 70000)   (numbers)
df.loc[high_salary, ['col_1', 'col_2', 'col_3']]   (specific columns)

// To check if contained in specific list //
countries = ['United States', 'India', 'United Kingdom', 'Germany', 'Canada']
filt = df['Country'].isin(countries)

// String Methods //
filt = df['LanguageWorkedWith'].str.contains('Python', na=False)



// MODIFYING COLUMNS //
df.columns = ['first_name', 'last_name', 'email']
df.columns = [x.upper() for x in df.columns]
df.columns = df.columns.str.replace(' ', '_')
# If I only want to change some columns
df.rename(columns = {'first_name': 'first', 'last_name': 'last'}, inplace=True)


// UPDATING DATA IN ROWS //
df.loc[2] = ['John', 'Smith', "JohnSmith@email.com"]
df.loc[2, ['last', 'email']] = ['Doe', 'JohnDoe@email.com']
df.loc[2, 'last'] = 'Smith'

with filters: 
filt = (df['email'] == 'JohnDoe@email.com')
df.loc[filt, 'last'] = 'Smith'

lowercasing, changing multiple rows at once:
df['email'] = df['email'].str.lower()


// APPLY //
(calling function on our values)
Series:
df['email'].apply(len)
df['email'].apply(<our_function>)
df['email'] = df['email'].apply(lambda x: x.lower())
DataFrames:
df.apply(len)
df.apply(len, axis='columns')
df.apply(lambda x: x.min())


// APPLYMAP //
(apply a function to every specific element in the dataframe)
df.applymap(len)
df.applymap(str.lower)


// MAP //
(substituting each vale in a series for another value)
(only on series)
(gives all not specified values a NaN value)
df['first'].map({'Corey': 'Chris', 'Jane': 'Mary'})


// REPLACE //
(avoids the NaN problem)
df['first'] = df['first'].replace({'Corey': 'Chris', 'Jane': 'Mary'})


// COMBINE COLUMNS VALUES //
df['first'] + ' ' + df['last']


// ADD COLUMN //
df['full_name'] = df['first'] + ' ' + df['last']


// REMOVE COLUMN //
df.drop(columns=['first', 'last'], inplace=True)


// SPLIT VALUES & ADD COLS (REVERSE PREVIOUS)//
df['full_name'].str.split(' ', expand=True)
df[['first', 'last']] = df['full_name'].str.split(' ', expand=True)


// ADDING ROWS // (APPEND IS DEPRECATED, USE CONCAT)
df2 = pd.DataFrame({'first': ['Tony']})
pd.concat([df, df2], ignore_index=True, sort=False)


// REMOVING ROWS //
df.drop(index=4)
# using filter:
filt = df['last'] == 'Doe'
df.drop(index=df[filt].index, inplace=True)


// SORTING //
df.sort_values(by='col_name')   ascending order
df.sort_values(by='col_name', ascending=False)   descending order

# sort multiple columns (when duplicate values exist)
df.sort_values(by=['col_1', 'col_2'], ascending=False)

# multiple columns but some in ascending and some in descending order
df.sort_values(by=['col_1', 'col_2'], ascending=[False, True], inplace=True)

# sort by index
df.sort_index()

# sort single column (series)
df['col_name'].sort_values()

# see highest/smallest something
df['col_name'].nlargest(10)  /  df['col_name'].nsmallest(10)
df.nlargest(10, 'col_name')   get all data


// STATISTICS //
df.describe()
df.median()
df['col_name'].median()
df['col_name'].count()
df['col_name'].value_counts(normailize=True)


// GROUPING //
smth_grp = df.groupby(['group_col_name'])  ie: 'Country'
smth_grp.get_group('from_group')  ie: 'India'
statistics:
smth_grp['col_name'].value_counts().head(50)
smth_grp['col_name'].value_counts(normalize=True).loc['from_group']
smth_grp['col_name'].median().loc['from_group']


// AGGREGATING //
smth_grp['col_name'].agg(['median', 'mean'])  get more than one function


// CLEANIING DATA //
df.dropna()
 # Default dropna settings
df.dropna(axis='index', how='any')
 # Only drops rows will all values missing
df.dropna(how='all')
 # drop columns with any missing values
df.dropna(axis='columns')
 # drop row if subset is missing
df.dropna(subset=['col_name'])
 # drop if both of the subsets are missing in a row
df.dropna(how='all', subset=['col_1', 'col_2'])


// DEALING WITH CUSTOMIZED MISSING VALUES //
 # when loading data
df = pd.DataFrame(data)
df.replace('Custom_NA', np.nan, inplace=True)
df.replace('Custom_Missing', np.nan, inplace=True)
 # csv file
na_vals = ['Custom_NA', 'Custom_Missing']
df = pd.read_csv('file.csv', index_col='optional', na_values=na_vals)


// CHECKING FOR MISSING DATA //
df.isna()


// FILLING MISSING VALUES IN //
df.fillna('MISSING')  or a number


// CASTING DATATYPES //
df.dtypes
type(np.nan)  is a float
df['age'] = df['age'].astype(float)


// LOOKING AT UNIQUE VALUES ON A SERIES //
df['col_name'].unique()


// DATES AND TIME SERIES DATA //
# to convert, we may need to format the date
# https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior
df['date_col'] = pd.to_datetime(df['date_col'], format='%Y-%m-%d %I-%p')

# now we can run date-time methods
df.loc[0, 'date_col'].day_name()

# we could have also formated the data when reading the csv
d_parser = lambda x: pd.to_datetime(x, format='%Y-%m-%d %I-%p')
df = pd.read_csv(link, parse_dates=['date_col'], date_parser=d_parser)

# we can access the whole dt series the same way we access str series:
df['date_col'].dt.day_name()

# we can quickly include this as a column
df['DayOfWeek'] = df['date_col'].dt.day_name()

# view earliest and latest dates in the data
df['date_col'].min()
df['date_col'].max()
df['date_col'].max() - df['Date'].min()   (span between two dates)

# filtering by date with strings
filt = (df['date_col'] >= '2019') & (df['date_col'] < '2020')
df.loc[filt]

# filtering by date with datetime
filt = (df['date_col'] >= pd.to_datetime('2019-01-01')) & (df['date_col'] < pd.to_datetime('2020-01-01'))
df.loc[filt]

# Slicing: if we set index to Date, then we can use slicing to filter
df.set_index('Date', inplace=True)
df.loc['2019']
df.loc['2020-01': '2020-02']

# avg specific qty within that timeframe
df.loc['2020-01': '2020-02', 'spec_col'].mean()

# RESAMPLING:

# resample data have data on a daily/weekly/monthly basis
# D, 1D, 2D, W, M
# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects
highs = df['High'].resample('D').max()
highs.loc['2020-01-01']

# resample multiple columns at once
df.resample('W').mean() (if same aggregation (not very useful))
# With various aggregations use AGG method
df.resample('W').agg({'Close': 'mean', 'High': 'max', 'Low': 'min', 'Volume': 'sum'})


//  Reading-Writing Data - CSV, Excel, JSON, SQL //
# Writing to a CSV
df.to_csv('data/modified.csv')

# **TAB delimited file
# Writing:
df.to_csv('data/modified.tsv', sep='\t')
# Reading:
df = pd.read_csv('data/modified.tsv', sep='\t')

# **EXCEL
# need to: pip install xlwt openpyxl xlrd (can run here with !pip or not)
# Writing (there are arguments to write to specific sheet and more)
df.to_excel('data/modified.xlsx')
# Reading
test = pd.read_excel('data/modified.xlsx', index_col='Respondent')

# **JSON
# Writing (default is Dictionary-like)
df.to_json('data/modified.json')
# If I wanted it List-like (better)
df.to_json('data/modified.json', orient='records', lines=True)
# Reading (depending on how the json looks we need to change orient and lines)
test = pd.read_json('data/modified.json', orient='records', lines=True)

# **SQL
# need to install: pip install SQLAlchemy
# and maybe also: pip install psycopg2-binary (not if using SQLite)
from sqlalchemy import create_engine
import psycopg2
# he asumes i have a database created with login and password
# do not put credentials in code, link to video on how to hide:
# https://www.youtube.com/watch?v=5iWhQWVXosU&ab_channel=CoreySchafer
# get a connection to DataBase (location here is Corey's)
engine = create_engine('postgresql://dbuser:dbpass@localhost:5432/sample_db')
# WRITING
# export to a table (doesn't need to exist yet)
# if_exists can do multiple different things
india_df.to_sql('sample_table', engine, if_exists='replace')
# READING
sql_df = pd.read_sql('sample_table', engine, index_col='Respondent')
# Instances when you dont need whole table but a query
sql_df = pd.read_sql_query('SELECT * FROM sample_table', engine, index_col='Respondent')
# **FROM URL
# just need to make sure you use the correct Method()
posts_df = pd.read_json('url/lalala/smth.json')